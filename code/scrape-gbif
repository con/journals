#!/usr/bin/env python

import requests
from bs4 import BeautifulSoup
import yaml

# URL of the GBIF data papers page
url = "https://www.gbif.org/data-papers"

# Send HTTP request and parse the HTML
response = requests.get(url)
soup = BeautifulSoup(response.content, 'html.parser')

# Find the table by its class name
table = soup.find('table', class_='table')

# Extract headers
headers = [th.get_text(strip=True) for th in table.find_all('th')]

# Extract table rows
data = []
names_map = {
    "Journal": "journal",
    "Publisher": "publisher",
    "Open Access (license)": "open_access_license",
    "APC estimate": "apc_estimate",
    "Journal Impact Factor (2022)": "journal_impact_factor",
    "Scopus CiteScore (2022)": "scopus_citescore",
    "Paper types": "paper_types"  # This field is added manually to each record
}

for tr in table.find_all('tr')[1:]:  # Skip the header row
    cells = tr.find_all('td')
    row = {}
    for i, cell in enumerate(cells):
        text = cell.get_text(strip=True)
        link = cell.find('a')
        url = link['href'] if link else None
        header = names_map[headers[i]]
        row[header] = {"value": text}
        if url:
            row[header]["url"] = url
    row["paper_types"] = ["DataPaper"]
    data.append(row)

# Write data to a YAML file
ofile = 'journals.yaml'
with open(ofile, 'w') as f:
    yaml.dump(data, f, allow_unicode=True)

print(f"Data scraped and saved to '{ofile}'")
